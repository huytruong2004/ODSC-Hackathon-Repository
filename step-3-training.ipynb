{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4c9fc31-b77e-4007-8901-d186024675cd",
   "metadata": {},
   "source": [
    "# Step 3: Training a LoRA Adapter\n",
    "\n",
    "This notebook performs the preparatory tasks needed for obtaining the base model that we will use for fine-tuning.\n",
    "\n",
    "This notebook showcases performing LoRA fine-tuning on the dataset that we curated in step 1.\n",
    "\n",
    "## Setup and Requirements\n",
    "Before proceeding, please make ensure you have completed the notebooks for steps 1 and 2. You will need to install one dependency to follow along. Execute the following cell before getting started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db9f6f9-b0c3-4c15-92d4-5c7ff0e5a286",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a8e64b",
   "metadata": {},
   "source": [
    "Let's also specify the base model name that we will use for fine-tuning. This should be the same model you downloaded/converted in step 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2328694",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_use = \"google/gemma-2-2b\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccfc187",
   "metadata": {},
   "source": [
    "---\n",
    "# Sanity Checking\n",
    "\n",
    "Let's do a quick sanity check to ensure we have all the pieces needed before moving forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11920d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "model_name = model_to_use.split('/')[-1].lower()\n",
    "\n",
    "# The path to the model checkpoint, and also the data directory containing the training, validation, and test data.\n",
    "nemo_model_fp = os.path.abspath(f\"models/{model_name}.nemo\")\n",
    "data_dir = \"data/split\"\n",
    "\n",
    "# The directory where the results will be stored.\n",
    "result_dir = os.path.abspath(\"results\")\n",
    "os.makedirs(result_dir, exist_ok=True)\n",
    "\n",
    "# Sanity checks\n",
    "assert os.path.exists(nemo_model_fp), f\"The model checkpoint at '{nemo_model_fp}' does not exist. Please ensure the model was downloaded successfully.\"\n",
    "assert os.path.exists(data_dir), f\"The data directory '{data_dir}' does not exist. Please ensure the data was prepared successfully.\"\n",
    "\n",
    "train_fp = os.path.abspath(f\"{data_dir}/train.jsonl\")\n",
    "val_fp = os.path.abspath(f\"{data_dir}/val.jsonl\")\n",
    "\n",
    "# Sanity checks\n",
    "assert os.path.exists(train_fp), f\"The training data at '{train_fp}' does not exist. Please ensure the data was prepared successfully.\"\n",
    "assert os.path.exists(val_fp), f\"The validation data at '{val_fp}' does not exist. Please ensure the data was prepared successfully.\"\n",
    "\n",
    "#\n",
    "# Set the environment variables (needed for executing the next cell)\n",
    "#\n",
    "%env BASE_MODEL=$nemo_model_fp\n",
    "%env DATA_DIR=$data_dir\n",
    "%env TRAIN_DS=$train_fp\n",
    "%env VAL_DS=$val_fp\n",
    "%env RESULT_DIR=$result_dir\n",
    "\n",
    "print(f\"\\n{'#'*80}\")\n",
    "print(\"All checks passed. You are ready to go!\")\n",
    "print(f\"    Base model file: {nemo_model_fp}\")\n",
    "print(f\"    Data directory: {data_dir}\")\n",
    "print(f\"    Results: {result_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d658d64",
   "metadata": {},
   "source": [
    "---\n",
    "# Model Training\n",
    "\n",
    "With all the sanity checks passing, it is time to start model training.\n",
    "\n",
    "> NOTE: Running the following cell will remove any previously trained model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4df27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "SCHEME=\"lora\"\n",
    "TP_SIZE=1\n",
    "PP_SIZE=1\n",
    "\n",
    "# Clear up cached mem-map file\n",
    "rm $DATA_DIR/*idx*\n",
    "# Clean up prior results\n",
    "rm -r $RESULT_DIR\n",
    "\n",
    "torchrun --nproc_per_node=1 \\\n",
    "/opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py \\\n",
    "    exp_manager.exp_dir=${RESULT_DIR} \\\n",
    "    exp_manager.explicit_log_dir=${RESULT_DIR} \\\n",
    "    trainer.devices=1 \\\n",
    "    trainer.num_nodes=1 \\\n",
    "    trainer.precision=bf16 \\\n",
    "    trainer.val_check_interval=200 \\\n",
    "    trainer.max_steps=1000 \\\n",
    "    trainer.gradient_clip_val=0.3 \\\n",
    "    model.megatron_amp_O2=True \\\n",
    "    ++model.mcore_gpt=True \\\n",
    "    model.tensor_model_parallel_size=${TP_SIZE} \\\n",
    "    model.pipeline_model_parallel_size=${PP_SIZE} \\\n",
    "    model.micro_batch_size=1 \\\n",
    "    model.global_batch_size=10 \\\n",
    "    model.peft.lora_tuning.adapter_dim=32\\\n",
    "    model.peft.lora_tuning.adapter_dropout=0.1\\\n",
    "    model.restore_from_path=${BASE_MODEL} \\\n",
    "    model.data.train_ds.num_workers=0 \\\n",
    "    model.data.train_ds.add_bos=True \\\n",
    "    model.data.validation_ds.num_workers=0 \\\n",
    "    model.data.train_ds.file_names=[${TRAIN_DS}] \\\n",
    "    model.data.train_ds.concat_sampling_probabilities=[1.0] \\\n",
    "    model.data.validation_ds.file_names=[${VAL_DS}] \\\n",
    "    model.peft.peft_scheme=${SCHEME}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0a1d15",
   "metadata": {},
   "source": [
    "---\n",
    "# Inference and Submission\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48e1b57",
   "metadata": {},
   "source": [
    "To make a submission, run inference with your model on the test dataset at `data/split/submission.jsonl`.\n",
    "\n",
    "> NOTE: This dataset was generated as part of Step 1. Please ensure it exists before proceeding.\n",
    "\n",
    "In order to do this, set the variable pointing to your submission data file in the set below, then excute the final cell.\n",
    "\n",
    "The inference results will be written under `results/inference` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c8569f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fp = os.path.abspath(f\"{data_dir}/submission.jsonl\")\n",
    "assert os.path.exists(test_fp), f\"The submission data at '{test_fp}' does not exist. Please ensure the data was prepared successfully.\"\n",
    "\n",
    "test_fp = 'ODSC-Hackathon-Repository/data/submission2.jsonl'\n",
    "adapter_fp = f\"{result_dir}/checkpoints/megatron_gpt_peft_lora_tuning.nemo\"\n",
    "os.makedirs(f\"{result_dir}/inference\", exist_ok=True)\n",
    "\n",
    "print(f\"Inference set: {test_fp}\")\n",
    "print(f\"Trained adapter: {adapter_fp}\")\n",
    "test_filename = os.path.basename(test_fp)\n",
    "\n",
    "\n",
    "%env TEST_DS=$test_fp\n",
    "%env TEST_FP=$test_filename\n",
    "%env TRAINED_ADAPTER=$adapter_fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445e222c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# This is where the inference results will be stored.\n",
    "OUTPUT_DIR=\"results/inference/infer-$TEST_FP\"\n",
    "\n",
    "SCHEME=\"lora\"\n",
    "TP_SIZE=1\n",
    "PP_SIZE=1\n",
    "\n",
    "# Clear up cached mem-map file\n",
    "rm $DATA_DIR/*idx*\n",
    "\n",
    "python /opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_generate.py \\\n",
    "    model.restore_from_path=${BASE_MODEL} \\\n",
    "    model.peft.restore_from_path=${TRAINED_ADAPTER} \\\n",
    "    trainer.devices=1 \\\n",
    "    trainer.num_nodes=1 \\\n",
    "    inference.greedy=True \\\n",
    "    model.data.test_ds.file_names=[${TEST_DS}] \\\n",
    "    model.data.test_ds.names=[\"infer\"] \\\n",
    "    model.data.test_ds.global_batch_size=16 \\\n",
    "    model.data.test_ds.micro_batch_size=1 \\\n",
    "    model.data.test_ds.tokens_to_generate=32 \\\n",
    "    model.tensor_model_parallel_size=${TP_SIZE} \\\n",
    "    model.pipeline_model_parallel_size=${PP_SIZE} \\\n",
    "    model.data.test_ds.output_file_path_prefix=$OUTPUT_DIR \\\n",
    "    model.data.test_ds.write_predictions_to_file=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d5e7ee",
   "metadata": {},
   "source": [
    "The results will be written under `results/inference`. Please send us this file for your final submission.\n",
    "\n",
    "Let's inspect a couple of lines from that file for sanity checking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4726de",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat results/inference/infer-submission.jsonl_test_infer_inputs_preds_labels.jsonl | head -n 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58554da2",
   "metadata": {},
   "source": [
    "---\n",
    "# Freeing Memory and Other Resources\n",
    "\n",
    "As always, it is a good idea to free up all allocated resources when you are done. Please execute the following cell to do so.\n",
    "\n",
    "Alternatively, please restart the kernel by navigating to `Kernel > Restart Kernel` (if using Jypyter notebook), or clicking the `Restart` button in VS Code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf196ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "exit(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
